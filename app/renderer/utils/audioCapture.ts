export type CaptureSource = 'display' | 'virtual-device' | 'loopback' | 'none';

export interface SystemAudioCaptureResult {
  stream: MediaStream | null;
  source: CaptureSource;
  warnings: string[];
  error?: string;
}

export interface SystemAudioCaptureOptions {
  platform?: NodeJS.Platform | 'unknown';
  /**
   * When true, try display capture first. macOS requires this to grab system audio.
   */
  preferDisplayCapture?: boolean;
  /**
   * Provide a way to log diagnostic messages without coupling to UI.
   */
  logger?: (message: string, data?: unknown) => void;
}

export interface MicrophoneCaptureOptions {
  deviceId?: string;
  echoCancellation?: boolean;
  noiseSuppression?: boolean;
  autoGainControl?: boolean;
  sampleRate?: number;
  logger?: (message: string, data?: unknown) => void;
}

const SYSTEM_AUDIO_KEYWORDS = [
  'stereo mix',
  'ç«‹é«”è²æ··éŸ³',
  'what u hear',
  'loopback',
  'cable',
  'voicemeeter',
  'virtual',
  'system'
];

const log = (logger?: (message: string, data?: unknown) => void, message?: string, data?: unknown) => {
  if (logger && message) {
    logger(message, data);
  } else if (message) {
    console.log(message, data ?? '');
  }
};

export const requestMicrophoneStream = async (
  options: MicrophoneCaptureOptions = {}
): Promise<MediaStream> => {
  if (!navigator.mediaDevices?.getUserMedia) {
    throw new Error('ç€è¦½å™¨ä¸æ”¯æ´éŸ³è¨Šæ“·å–');
  }

  const {
    deviceId,
    echoCancellation = true,
    noiseSuppression = true,
    autoGainControl = true,
    sampleRate = 44100
  } = options;

  log(options.logger, 'ğŸ™ï¸ æ­£åœ¨è«‹æ±‚éº¥å…‹é¢¨ä¸²æµ');

  return navigator.mediaDevices.getUserMedia({
    audio: {
      deviceId,
      echoCancellation,
      noiseSuppression,
      autoGainControl,
      sampleRate
    },
    video: false
  });
};

export const requestSystemAudioStream = async (
  options: SystemAudioCaptureOptions = {}
): Promise<SystemAudioCaptureResult> => {
  const platform = options.platform ?? 'unknown';
  const preferDisplayCapture = options.preferDisplayCapture ?? (platform === 'darwin' || platform === 'win32');
  const warnings: string[] = [];

  const attemptDesktopCapture = async (sourceId: string) => {
    const constraints: any = {
      audio: {
        mandatory: {
          chromeMediaSource: 'desktop',
          chromeMediaSourceId: sourceId
        }
      },
      video: {
        mandatory: {
          chromeMediaSource: 'desktop',
          chromeMediaSourceId: sourceId,
          maxWidth: 1,
          maxHeight: 1,
          maxFrameRate: 1
        }
      }
    };

    log(options.logger, 'ğŸ–¥ï¸ å˜—è©¦é€éæ¡Œé¢ä¾†æºæ“·å–ç³»çµ±è²éŸ³', sourceId);

    const desktopStream = await navigator.mediaDevices.getUserMedia(constraints);
    const audioTracks = desktopStream.getAudioTracks();

    if (audioTracks.length === 0) {
      desktopStream.getTracks().forEach(track => track.stop());
      throw new Error('æ¡Œé¢ä¾†æºæœªæä¾›éŸ³è¨Šè»Œé“');
    }

    desktopStream.getVideoTracks().forEach(track => track.stop());
    const audioStream = new MediaStream();
    audioTracks.forEach(track => audioStream.addTrack(track));

    return audioStream;
  };

  if (typeof window !== 'undefined' && (window as any)?.electronAPI?.getAudioSources) {
    try {
      const sources = await (window as any).electronAPI.getAudioSources();
      const candidateSources: Array<{ id: string; name: string }> = Array.isArray(sources) ? sources : [];

      const preferredOrder = candidateSources.filter(source => {
        if (!source?.id) return false;
        if (platform === 'darwin') {
          return source.id.startsWith('screen:');
        }
        return source.id.startsWith('screen:') || source.id.startsWith('window:');
      });

      for (const source of [...preferredOrder, ...candidateSources]) {
        try {
          if (!source?.id) {
            continue;
          }
          const audioStream = await attemptDesktopCapture(source.id);
          return {
            stream: audioStream,
            source: 'display',
            warnings
          };
        } catch (error) {
          const message = error instanceof Error ? error.message : String(error);
      if (message?.toLowerCase().includes('denied') || message?.toLowerCase().includes('not allowed')) {
        warnings.push('è«‹ç¢ºèªå·²åœ¨ç³»çµ±åå¥½è¨­å®šä¸­æˆæ¬Šè¢å¹•éŒ„è£½/éº¥å…‹é¢¨æ¬Šé™çµ¦ Meeting Recorder');
      } else if (message?.toLowerCase().includes('not supported')) {
        warnings.push('ç›®å‰ç’°å¢ƒä¸æ”¯æ´å¾æ¡Œé¢ä¾†æºæ“·å–éŸ³è¨Šï¼ˆå¯èƒ½ç¼ºå°‘è¢å¹•éŒ„è£½æ¬Šé™ï¼‰');
      } else {
        warnings.push(`ç„¡æ³•é€éä¾†æº ${source?.name ?? source?.id} æ“·å–ï¼š${message}`);
      }
      log(options.logger, 'âš ï¸ æ¡Œé¢ä¾†æºæ“·å–å¤±æ•—', { source, error });
        }
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      warnings.push(`ç„¡æ³•å–å¾—æ¡Œé¢ä¾†æºåˆ—è¡¨ï¼š${message}`);
      log(options.logger, 'âŒ ç„¡æ³•å–å¾—æ¡Œé¢ä¾†æºåˆ—è¡¨', error);
    }
  }

  // Try display capture (works on macOS and modern Windows builds)
  if (preferDisplayCapture && navigator.mediaDevices?.getDisplayMedia) {
    try {
      log(options.logger, 'ğŸ–¥ï¸ å˜—è©¦é€éè¢å¹•éŒ„è£½å–å¾—ç³»çµ±è²éŸ³');
      const displayConstraints: DisplayMediaStreamOptions = {
        audio: true,
        video: platform === 'darwin' ? { width: 1, height: 1, frameRate: 1 } : false
      };

      const displayStream = await navigator.mediaDevices.getDisplayMedia(displayConstraints);
      const audioTracks = displayStream.getAudioTracks();

      if (audioTracks.length > 0) {
        log(options.logger, 'âœ… å·²å–å¾—è¢å¹•éŸ³è¨Šè»Œé“', audioTracks.map(track => ({
          id: track.id,
          label: track.label,
          kind: track.kind
        })));

        // Stop all video tracks immediately â€“ we only care about audio
        displayStream.getVideoTracks().forEach(track => track.stop());

        const audioStream = new MediaStream();
        audioTracks.forEach(track => audioStream.addTrack(track));

        return {
          stream: audioStream,
          source: 'display',
          warnings
        };
      }

      warnings.push('è¢å¹•éŒ„è£½æœªæä¾›ä»»ä½•éŸ³è¨Šè»Œé“');
      displayStream.getTracks().forEach(track => track.stop());
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      warnings.push(`è¢å¹•éŒ„è£½å¤±æ•—ï¼š${message}`);
      log(options.logger, 'âŒ è¢å¹•éŒ„è£½å–å¾—ç³»çµ±è²éŸ³å¤±æ•—', error);
    }
  }

  // Fallback: look for virtual/loopback audio input devices (primarily Windows)
  if (navigator.mediaDevices?.enumerateDevices) {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(device => device.kind === 'audioinput');

      const virtualDevice = audioInputs.find(device => {
        const label = device.label.toLowerCase();
        return SYSTEM_AUDIO_KEYWORDS.some(keyword => label.includes(keyword));
      });

      if (virtualDevice) {
        log(options.logger, 'ğŸ§ ä½¿ç”¨è™›æ“¬éŸ³è¨Šè¨­å‚™æ“·å–ç³»çµ±è²éŸ³', virtualDevice.label);
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: { exact: virtualDevice.deviceId },
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false
          },
          video: false
        });

        return {
          stream,
          source: 'virtual-device',
          warnings
        };
      }

      warnings.push('æœªåµæ¸¬åˆ°è™›æ“¬éŸ³è¨Šæˆ–ç«‹é«”è²æ··éŸ³è¨­å‚™');
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      warnings.push(`åµæ¸¬éŸ³è¨Šè¨­å‚™å¤±æ•—ï¼š${message}`);
      log(options.logger, 'âŒ åµæ¸¬éŸ³è¨Šè¨­å‚™å¤±æ•—', error);
    }
  } else {
    warnings.push('ç•¶å‰ç’°å¢ƒä¸æ”¯æ´è£ç½®åµæ¸¬');
  }

  return {
    stream: null,
    source: 'none',
    warnings,
    error: warnings[warnings.length - 1]
  };
};

export const mergeMediaStreams = (streams: MediaStream[]): MediaStream => {
  const audioContext = new AudioContext();
  const destination = audioContext.createMediaStreamDestination();

  streams.forEach(stream => {
    if (stream.getAudioTracks().length > 0) {
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(destination);
    }
  });

  return destination.stream;
};

export const stopStream = (stream: MediaStream | null | undefined): void => {
  if (!stream) {
    return;
  }

  stream.getTracks().forEach(track => {
    track.stop();
  });
};
